import os
import time
import numpy as np
import pandas as pd
import gensim
from tqdm import tqdm
from nltk.stem import PorterStemmer

ps = PorterStemmer()
from nltk.stem.lancaster import LancasterStemmer

lc = LancasterStemmer()
from nltk.stem import SnowballStemmer

sb = SnowballStemmer("english")
import gc
import pickle
import re
import spacy
from pathlib import Path

symbols_to_delete = '\nğŸ•\rğŸµğŸ˜‘\xa0\ue014\t\uf818\uf04a\xadğŸ˜¢ğŸ¶ï¸\uf0e0ğŸ˜œğŸ˜ğŸ‘Š\u200b\u200eğŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ğŸ˜ğŸ’–ğŸ’µĞ•ğŸ‘ğŸ˜€ğŸ˜‚\u202a\u202cğŸ”¥ğŸ˜„ğŸ»ğŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ğŸ˜‹ğŸ‘×©×œ×•××‘×™ğŸ˜±â€¼\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\u2009ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ™€ğŸ˜ğŸ˜•\u200fğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜××¢×›×—ğŸ’©ğŸ’¯â›½ğŸš„ğŸ¼à®œğŸ˜–á´ ğŸš²â€ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡\x7fğŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ğŸ™„ï¼¨ğŸ˜ \ufeff\u2028ğŸ˜‰ğŸ˜¤â›ºğŸ™‚\u3000ØªØ­ÙƒØ³Ø©ğŸ‘®ğŸ’™ÙØ²Ø·ğŸ˜ğŸ¾ğŸ‰ğŸ˜\u2008ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ª\x08â€‘ğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğ˜Šğ˜¦ğ˜³ğ˜¢ğ˜µğ˜°ğ˜¤ğ˜ºğ˜´ğ˜ªğ˜§ğ˜®ğ˜£ğŸ’—ğŸ’šåœ°ç„è°·ÑƒĞ»ĞºĞ½ĞŸĞ¾ĞĞğŸ¾ğŸ•ğŸ˜†×”ğŸ”—ğŸš½æ­Œèˆä¼ğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸Ğ¼Ï…Ñ‚Ñ•â¤µğŸ†ğŸƒğŸ˜©\u200ağŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ÑĞ¿Ñ€Ğ´\x95ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™Œ\u2002ğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\x13ğŸš¬ğŸ¤“\ue602ğŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª××“×£× ×¨×š×¦×˜ğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“\uf0b7\uf04c\x9f\x10æˆéƒ½ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ĞµÑ…ğŸ˜²á¼¸á¾¶á½ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘\u202dğŸ’¤ğŸ‡\ue613å°åœŸè±†ğŸ¡â”â‰\u202fğŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ğŸ‡¹ğŸ‡¼ğŸŒ¸è”¡è‹±æ–‡ğŸŒğŸ²ãƒ¬ã‚¯ã‚µã‚¹ğŸ˜›å¤–å›½äººå…³ç³»Ğ¡Ğ±ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙÑŒÑ‹Ğ³Ñä¸æ˜¯\x9c\x9dğŸ—‘\u2005ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°á¸·Ğ—Ğ·â–±Ñ†ï¿¼ğŸ¤£å–æ¸©å“¥åè®®ä¼šä¸‹é™ä½ å¤±å»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨éª—å­ğŸãƒ„ğŸ…\x85ğŸºØ¢Ø¥Ø´Ø¡ğŸµğŸŒÍŸá¼”æ²¹åˆ«å…‹ğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§Ğ¹\u2003ğŸš€ğŸ¤´Ê²ÑˆÑ‡Ğ˜ĞĞ Ğ¤Ğ”Ğ¯ĞœÑĞ¶ğŸ˜ğŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ğŸ’¨åœ†æ˜å›­×§â„ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦\u200dğ“’ğ“²ğ“¿ğ“µì•ˆì˜í•˜ì„¸ìš”Ğ–Ñ™ĞšÑ›ğŸ€ğŸ˜«ğŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªå¤©ä¸€å®¶âš²\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ‡®ğŸ‡¬ğŸ‡§ğŸ˜·ğŸ‡¨ğŸ‡¦Ğ¥Ğ¨ğŸŒ\x1fæ€é¸¡ç»™çŒ´çœ‹Êğ—ªğ—µğ—²ğ—»ğ˜†ğ—¼ğ˜‚ğ—¿ğ—®ğ—¹ğ—¶ğ˜‡ğ—¯ğ˜ğ—°ğ˜€ğ˜…ğ—½ğ˜„ğ—±ğŸ“ºÏ–\u2000Ò¯Õ½á´¦á¥Ò»Íº\u2007Õ°\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆğ“ğ¡ğğ«ğ®ğğšğƒğœğ©ğ­ğ¢ğ¨ğ§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ğ†á´‘Üğ¬ğ°ğ²ğ›ğ¦ğ¯ğ‘ğ™ğ£ğ‡ğ‚ğ˜ğŸÔœĞ¢á—à±¦ã€”á«ğ³ğ”ğ±ğŸ”ğŸ“ğ…ğŸ‹ï¬ƒğŸ’˜ğŸ’“Ñ‘ğ˜¥ğ˜¯ğ˜¶ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğ™¬ğ™–ğ™¨ğ™¤ğ™£ğ™¡ğ™®ğ™˜ğ™ ğ™šğ™™ğ™œğ™§ğ™¥ğ™©ğ™ªğ™—ğ™ğ™ğ™›ğŸ‘ºğŸ·â„‹ğ€ğ¥ğªğŸš¶ğ™¢á¼¹ğŸ¤˜Í¦ğŸ’¸Ø¬íŒ¨í‹°ï¼·ğ™‡áµ»ğŸ‘‚ğŸ‘ƒÉœğŸ«\uf0a7Ğ‘Ğ£Ñ–ğŸš¢ğŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ğŸƒğ“¬ğ“»ğ“´ğ“®ğ“½ğ“¼â˜˜ï´¾Ì¯ï´¿â‚½\ue807ğ‘»ğ’†ğ’ğ’•ğ’‰ğ’“ğ’–ğ’‚ğ’ğ’…ğ’”ğ’ğ’—ğ’ŠğŸ‘½ğŸ˜™\u200cĞ›â€’ğŸ¾ğŸ‘¹âŒğŸ’â›¸å…¬å¯“å…»å® ç‰©å—ğŸ„ğŸ€ğŸš‘ğŸ¤·æ“ç¾ğ’‘ğ’šğ’ğ‘´ğŸ¤™ğŸ’æ¬¢è¿æ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ğ™«ğŸˆğ’Œğ™Šğ™­ğ™†ğ™‹ğ™ğ˜¼ğ™…ï·»ğŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ğŸš—ğŸ³ğŸğŸğŸ–ğŸ‘ğŸ•ğ’„ğŸ—ğ ğ™„ğ™ƒğŸ‘‡é”Ÿæ–¤æ‹·ğ—¢ğŸ³ğŸ±ğŸ¬â¦ãƒãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ğ˜¿ğ™”â‚µğ’©â„¯ğ’¾ğ“ğ’¶ğ“‰ğ“‡ğ“Šğ“ƒğ“ˆğ“…â„´ğ’»ğ’½ğ“€ğ“Œğ’¸ğ“ğ™Î¶ğ™Ÿğ˜ƒğ—ºğŸ®ğŸ­ğŸ¯ğŸ²ğŸ‘‹ğŸ¦Šå¤šä¼¦ğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸å…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸæ‰¹åˆ¤æ£€è®¨ğŸğŸ¦ğŸ™‹ğŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ì˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ğŸ”«ğŸ‘å‡¸á½°ğŸ’²ğŸ—¯ğ™ˆá¼Œğ’‡ğ’ˆğ’˜ğ’ƒğ‘¬ğ‘¶ğ•¾ğ–™ğ–—ğ–†ğ–ğ–Œğ–ğ–•ğ–Šğ–”ğ–‘ğ–‰ğ–“ğ–ğ–œğ–ğ–šğ–‡ğ•¿ğ–˜ğ–„ğ–›ğ–’ğ–‹ğ–‚ğ•´ğ–Ÿğ–ˆğ•¸ğŸ‘‘ğŸš¿ğŸ’¡çŸ¥å½¼ç™¾\uf005ğ™€ğ’›ğ‘²ğ‘³ğ‘¾ğ’‹ğŸ’ğŸ˜¦ğ™’ğ˜¾ğ˜½ğŸğ˜©ğ˜¨á½¼á¹‘ğ‘±ğ‘¹ğ‘«ğ‘µğ‘ªğŸ‡°ğŸ‡µğŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘á“€á£ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ç‚¹å‡»æŸ¥ç‰ˆğŸ­ğ‘¥ğ‘¦ğ‘§ï¼®ï¼§ğŸ‘£\uf020ã£ğŸ‰Ñ„ğŸ’­ğŸ¥ÎğŸ´ğŸ‘¨ğŸ¤³ğŸ¦\x0bğŸ©ğ‘¯ğ’’ğŸ˜—ğŸğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²Ú†ÛŒğ‘®ğ—•ğ—´ğŸ’êœ¥â²£â²ğŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ğŸ’Šã€Œã€\uf203\uf09a\uf222\ue608\uf202\uf099\uf469\ue607\uf410\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆĞ“ğ‘©ğ‘°ğ’€ğ‘ºğŸŒ¤ğ—³ğ—œğ—™ğ—¦ğ—§ğŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ğŸ‡³ğ’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ğ’ğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğ˜¬ğ˜±ğ˜¸ğ˜·ğ˜ğ˜­ğ˜“ğ˜–ğ˜¹ğ˜²ğ˜«Ú©Î’ÏğŸ’¢ÎœÎŸÎÎ‘Î•ğŸ‡±â™²ğˆâ†´ğŸ’’âŠ˜È»ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğğğŠğ‘­ğŸ¤–ğŸğŸ˜¼ğŸ•·ï½‡ï½’ï½ï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ğŸ°ğŸ‡´ğŸ‡­ğŸ‡»ğŸ‡²ğ—ğ—­ğ—˜ğ—¤ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸ\uf10aáƒšÚ¡ğŸ¦\U0001f92f\U0001f92ağŸ¡ğŸ’³á¼±ğŸ™‡ğ—¸ğ—Ÿğ— ğ—·ğŸ¥œã•ã‚ˆã†ãªã‚‰ğŸ”¼'
remove_dict = {ord(c): f'' for c in symbols_to_delete}

symbols_to_isolate = ',?!-;*"â€¦:â€”()%#$&_/@=^<>[]'
isolate_dict = {ord(c): f' {c} ' for c in symbols_to_isolate}

spell_model = gensim.models.KeyedVectors.load_word2vec_format('../input/wikinews300d1mvec/wiki-news-300d-1M.vec')
words = spell_model.index2word
w_rank = {}
for i, word in enumerate(words):
    w_rank[word] = i
WORDS = w_rank


# Use fast text as vocabulary
def words(text): return re.findall(r'\w+', text.lower())


def P(word):
    "Probability of `word`."
    # use inverse of rank as proxy
    # returns 0 if the word isn't in the dictionary
    return - WORDS.get(word, 0)


def correction(word):
    "Most probable spelling correction for word."
    return max(candidates(word), key=P)


def candidates(word):
    "Generate possible spelling corrections for word."
    return (known([word]) or known(edits1(word)) or [word])


def known(words):
    "The subset of `words` that appear in the dictionary of WORDS."
    return set(w for w in words if w in WORDS)


def edits1(word):
    "All edits that are one edit away from `word`."
    letters = 'abcdefghijklmnopqrstuvwxyz'
    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
    deletes = [L + R[1:] for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
    inserts = [L + c + R for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)


def edits2(word):
    "All edits that are two edits away from `word`."
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))


def singlify(word):
    return "".join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])


def load_myemb(word_dict, lemma_dict):
    EMBEDDING_FILE = '../input/myembedding/myembedding.pkl'
    with open(EMBEDDING_FILE, 'rb') as f:
        embeddings_index = pickle.load(f)
    embed_size = 100
    nb_words = len(word_dict) + 10
    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)
    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.
    print(unknown_vector[:5])
    for key in tqdm(word_dict):
        word = key
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.lower()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.upper()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.capitalize()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = ps.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lc.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = sb.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lemma_dict[key]
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        # if len(key) > 1:
        #     word = correction(key)
        #     embedding_vector = embeddings_index.get(word)
        #     if embedding_vector is not None:
        #         embedding_matrix[word_dict[key]] = embedding_vector
        #         continue
        embedding_matrix[word_dict[key]] = unknown_vector
    return embedding_matrix, nb_words


def load_glove(word_dict, lemma_dict):
    EMBEDDING_FILE = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'
    with open(EMBEDDING_FILE, 'rb') as f:
        embeddings_index = pickle.load(f)
    embed_size = 300
    nb_words = len(word_dict) + 10
    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)
    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.
    print(unknown_vector[:5])
    for key in tqdm(word_dict):
        word = key
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.lower()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.upper()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.capitalize()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = ps.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lc.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = sb.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lemma_dict[key]
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        if len(key) > 1:
            word = correction(key)
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[word_dict[key]] = embedding_vector
                continue
        embedding_matrix[word_dict[key]] = unknown_vector
    return embedding_matrix, nb_words


def load_crawl(word_dict, lemma_dict):
    EMBEDDING_FILE = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'
    with open(EMBEDDING_FILE, 'rb') as f:
        embeddings_index = pickle.load(f)
    embed_size = 300
    nb_words = len(word_dict) + 10
    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)
    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.
    print(unknown_vector[:5])
    for key in tqdm(word_dict):
        word = key
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.lower()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.upper()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = key.capitalize()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = ps.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lc.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = sb.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lemma_dict[key]
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        if len(key) > 1:
            word = correction(key)
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[word_dict[key]] = embedding_vector
                continue
        embedding_matrix[word_dict[key]] = unknown_vector
    return embedding_matrix, nb_words


def handle_punctuation(x):
    x = x.translate(remove_dict)
    x = x.translate(isolate_dict)
    return x


start_time = time.time()
print("Loading data ...")
train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')
test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')  # .fillna(' ')
train_text = train['comment_text']  # .apply(lambda x:handle_punctuation(x))
test_text = test['comment_text']
text_list = pd.concat([train_text, test_text])
y = train['target'].values
num_train_data = y.shape[0]
print("--- %s seconds ---" % (time.time() - start_time))

start_time = time.time()
print("Spacy NLP ...")
nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner', 'tagger'])
nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)
word_dict = {}
word_index = 2  # 0 for pad token, 1 for unknown
lemma_dict = {}
docs = nlp.pipe(text_list, n_threads=8, batch_size=128)
word_sequences = []
for doc in tqdm(docs):
    word_seq = []
    for token in doc:
        if token.is_space: continue
        if token.like_url:
            t = 'url'
        elif token.like_num:
            t = '100'
        elif token.like_email:
            t = 'email'
        else:
            t = token.text

        if (t not in word_dict) and (token.pos_ is not "PUNCT"):
            word_dict[t] = word_index
            word_index += 1
            lemma_dict[t] = token.lemma_
        if token.pos_ is not "PUNCT":
            word_seq.append(t)
    while len(word_seq) < 2: word_seq.append('.')
    word_sequences.append(word_seq)
del docs
gc.collect()
train_word_sequences = word_sequences[:num_train_data]
test_word_sequences = word_sequences[num_train_data:]

save_dir = Path('../input/preparedRNNData')
save_dir.mkdir(exist_ok=True, parents=True)

with open(save_dir / 'word_dict.pkl', 'wb') as f:
    pickle.dump(word_dict, f)
with open(save_dir / 'lemma_dict.pkl', 'wb') as f:
    pickle.dump(lemma_dict, f)
with open(save_dir / 'training_data.pkl', 'wb') as f:
    pickle.dump(train_word_sequences, f)
print("--- %s seconds ---" % (time.time() - start_time))

from gensim.models import Word2Vec

NewdataEmbedding = Word2Vec(word_sequences, size=100, window=5, min_count=1, workers=8, iter=5, seed=2019)  # è®­ç»ƒ100ç»´çš„

output_dir = Path('../input/myembedding')
output_dir.mkdir(exist_ok=True, parents=True)

embedding_dict = {w: NewdataEmbedding.wv[w] for w in NewdataEmbedding.wv.vocab.keys()}

with open(output_dir / 'myembedding.pkl', 'wb+') as f:
    pickle.dump(embedding_dict, f)
print("--- %s seconds ---" % (time.time() - start_time))

glove_embedding_matrix_glove, nb_words = load_glove(word_dict, lemma_dict)
print(nb_words)
crawl_embedding_matrix_glove, nb_words = load_crawl(word_dict, lemma_dict)
my_embedding_matrix, nb_words = load_myemb(word_dict, lemma_dict)

print("--- %s seconds ---" % (time.time() - start_time))

with open(save_dir / 'glove_embedding.npy', 'wb') as f:
    np.save(f, glove_embedding_matrix_glove)

with open(save_dir / 'crawl_embedding.npy', 'wb') as f:
    np.save(f, crawl_embedding_matrix_glove)
with open(save_dir / 'my_embedding.npy', 'wb') as f:
    np.save(f, my_embedding_matrix)
